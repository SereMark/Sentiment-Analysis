{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e8547d1",
   "metadata": {},
   "source": [
    "The task if to implement a sentiment analysis project, where you should aim to systematically explore and document the task according to the criteria laid out in the task. Here are the detailed steps tailored to your sentiment analysis task:\n",
    "\n",
    "1. Define the Problem and Objective\n",
    "Objective: Classify sentiments of IMDb movie reviews using vector-based text representations. The goal is to determine the efficacy of different embeddings and model architectures (static vs. contextual) in sentiment classification.\n",
    "Hypothesis: Contextual embeddings (like BERT) will perform significantly better than static embeddings (like Word2Vec) for sentiment classification in movie reviews.\n",
    "2. Data Acquisition and Preprocessing\n",
    "Dataset: Use the IMDb movie reviews dataset.\n",
    "Preprocessing:\n",
    "Tokenization and normalization (convert to lowercase, remove punctuation).\n",
    "Remove HTML tags as IMDb reviews often contain such artifacts.\n",
    "Ensure text data is prepared appropriately for both types of embeddings (static and contextual).\n",
    "3. Model Selection and Configuration\n",
    "Static Model Setup: Utilize pre-trained Word2Vec embeddings to transform text data into vectors and feed these into a simple neural network or SVM for classification.\n",
    "Contextual Model Setup: Employ a pre-trained BERT model from the Hugging Face library, fine-tuning it on the sentiment classification task.\n",
    "Baseline Model: As a baseline, implement a naive approach that classifies every review as positive (since many datasets have a high prevalence of positive reviews).\n",
    "4. Experimentation and Evaluation\n",
    "Experiment with Hyperparameters:\n",
    "Adjust learning rates and batch sizes for both the static and contextual models.\n",
    "Experiment with different network architectures for the neural network using Word2Vec embeddings.\n",
    "Input Generation Strategies: For the BERT model, experiment with different ways of presenting the text:\n",
    "Single sentence vs. review snippet (first few sentences).\n",
    "With and without additional preprocessing like stemming.\n",
    "Metrics: Evaluate the models using accuracy, precision, recall, and F1-score. Consider also using ROC-AUC if the classification threshold needs tuning.\n",
    "5. Documentation and Reporting\n",
    "Jupyter Notebook: Document all steps, from data preprocessing to model training and evaluation. Use markdown cells to explain the purpose and outcome of each block of code.\n",
    "Visualization: Include charts and tables to visually represent the performance comparison between models. Use confusion matrices and ROC curves where applicable.\n",
    "Results Analysis: Discuss the performance differences observed between static and contextual embeddings, and between different hyperparameter settings.\n",
    "Conclusions and Future Work: Summarize the findings and propose future directions for extending this work. Could other embeddings or models improve performance?\n",
    "6. Project Presentation\n",
    "Prepare a detailed presentation outlining your methodology, experiments, results, and conclusions. Make sure to tailor this presentation to your audience, focusing on the critical insights and what you've learned about NLP through this project.\n",
    "\n",
    "# Sentiment Analysis of IMDb Reviews\n",
    "\n",
    "\n",
    "This notebook implements a sentiment analysis task using IMDb movie reviews. We compare the effectiveness of static embeddings (Word2Vec) and contextual embeddings (BERT) in classifying sentiments expressed in movie reviews.\n",
    "\n",
    "### 1. Setup and Installation\n",
    "\n",
    "First, we import necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0e7f889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in c:\\users\\serem\\appdata\\roaming\\python\\python312\\site-packages (1.12.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\serem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\serem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: transformers in c:\\users\\serem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.41.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\serem\\appdata\\roaming\\python\\python312\\site-packages (1.5.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\serem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.0)\n",
      "Requirement already satisfied: seaborn in c:\\users\\serem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: threadpoolctl in c:\\users\\serem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.5.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\serem\\appdata\\roaming\\python\\python312\\site-packages (1.4.2)\n",
      "Requirement already satisfied: gensim in c:\\users\\serem\\appdata\\roaming\\python\\python312\\site-packages (4.3.2)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\serem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (8.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\serem\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\serem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\serem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\serem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (3.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in c:\\users\\serem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.23.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\serem\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\serem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\serem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in c:\\users\\serem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.32.2)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\serem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\serem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\serem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\serem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\serem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\serem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\serem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\serem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\serem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\serem\\appdata\\roaming\\python\\python312\\site-packages (from gensim) (7.0.4)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\serem\\appdata\\roaming\\python\\python312\\site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\serem\\appdata\\roaming\\python\\python312\\site-packages (from ipywidgets) (8.24.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\serem\\appdata\\roaming\\python\\python312\\site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.10 in c:\\users\\serem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipywidgets) (4.0.10)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.10 in c:\\users\\serem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ipywidgets) (3.0.10)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\serem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\serem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\serem\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\serem\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\serem\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\serem\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\serem\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\n",
      "Requirement already satisfied: stack-data in c:\\users\\serem\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\serem\\appdata\\roaming\\python\\python312\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\serem\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\serem\\appdata\\roaming\\python\\python312\\site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\serem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\serem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\serem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\serem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\serem\\appdata\\roaming\\python\\python312\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\serem\\appdata\\roaming\\python\\python312\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\serem\\appdata\\roaming\\python\\python312\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\serem\\appdata\\roaming\\python\\python312\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\serem\\appdata\\roaming\\python\\python312\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Requirement already satisfied: torch in c:\\users\\serem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.3.0+cu121)\n",
      "Requirement already satisfied: torchvision in c:\\users\\serem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.18.0+cu121)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\serem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.3.0+cu121)\n",
      "Requirement already satisfied: filelock in c:\\users\\serem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\serem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.12.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\serem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\serem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\serem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\serem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2024.5.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\serem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2021.4.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\serem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\serem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\serem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\serem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.12.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\serem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\serem\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Installation of necessary libraries with user flags\n",
    "%pip install scipy pandas numpy transformers scikit-learn matplotlib seaborn threadpoolctl joblib gensim ipywidgets\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from gensim import downloader as api\n",
    "\n",
    "# Check for CUDA availability for GPU acceleration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set up random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b50a845",
   "metadata": {},
   "source": [
    "### 2. Problem Definition and Hypothesis\n",
    "#### Problem Definition\n",
    "The objective of this project is to classify sentiments of IMDb movie reviews using vector-based text representations. Specifically, we aim to determine the efficacy of different embeddings and model architectures (static vs. contextual) in sentiment classification.\n",
    "\n",
    "#### Hypothesis\n",
    "We hypothesize that contextual embeddings (like BERT) will perform significantly better than static embeddings (like Word2Vec) for sentiment classification in movie reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8ffad1",
   "metadata": {},
   "source": [
    "### 3. Data Acquisition and Preprocessing\n",
    "We load the IMDb dataset and preprocess the text by removing HTML tags, converting to lowercase, and removing punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2e49595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the IMDb movie reviews dataset\n",
    "df = pd.read_csv('C:/Users/serem/Documents/Workspaces/Sentiment Analysis/IMDB Dataset.csv')\n",
    "df['review'] = df['review'].apply(lambda x: re.sub(r'<[^>]*>', '', x).lower().replace(r'[^\\w\\s]', ' ').strip())\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(df['review'], df['sentiment'].map({'positive': 1, 'negative': 0}), test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenization for BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize_for_bert(texts):\n",
    "    return tokenizer(texts, padding='max_length', truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "train_encodings = tokenize_for_bert(train_texts.tolist())\n",
    "val_encodings = tokenize_for_bert(val_texts.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7889f9fa",
   "metadata": {},
   "source": [
    "### 4. Model Configuration\n",
    "#### Static Embeddings with Word2Vec\n",
    "We utilize pre-trained Word2Vec embeddings to transform text data into vectors and feed these into a simple neural network for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b01719c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained Word2Vec\n",
    "word_vectors = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "# Model for Word2Vec embeddings\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(300, 50)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(50, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "static_model = SentimentClassifier().to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca0ad9b",
   "metadata": {},
   "source": [
    "#### Contextual Embeddings with BERT\n",
    "We employ a pre-trained BERT model from the Hugging Face library, fine-tuning it on the sentiment classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8622244",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load BERT model\n",
    "bert_model = BertForSequenceClassification.from_pretrained('bert-base-uncased').to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb0d588",
   "metadata": {},
   "source": [
    "### 5. Experimentation\n",
    "We perform training and validation for both models.\n",
    "\n",
    "#### Define a Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50405a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dataloader\n",
    "class IMDbDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "train_dataset = IMDbDataset(train_encodings, train_labels.tolist())\n",
    "val_dataset = IMDbDataset(val_encodings, val_labels.tolist())\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929df21f",
   "metadata": {},
   "source": [
    "#### Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d32099d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train(model, data_loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02aeebd4",
   "metadata": {},
   "source": [
    "#### 6. Model Training\n",
    "Train the BERT model and validate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7184b264",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\serem\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizers initialized.\n",
      "Using device: cuda\n",
      "Starting Epoch 1\n",
      "Training phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\serem\\AppData\\Local\\Temp\\ipykernel_15260\\450977610.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "c:\\Users\\serem\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:435: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished training for Epoch 1, Train Loss: 0.2100180212739855\n",
      "Evaluation phase...\n",
      "Finished evaluation for Epoch 1, Val Loss: 0.15192935450077058\n",
      "Epoch 1, Train Loss: 0.2100180212739855, Val Loss: 0.15192935450077058\n",
      "Starting Epoch 2\n",
      "Training phase...\n",
      "Finished training for Epoch 2, Train Loss: 0.11100937440088018\n",
      "Evaluation phase...\n",
      "Finished evaluation for Epoch 2, Val Loss: 0.1637596469003707\n",
      "Epoch 2, Train Loss: 0.11100937440088018, Val Loss: 0.1637596469003707\n",
      "Starting Epoch 3\n",
      "Training phase...\n",
      "Finished training for Epoch 3, Train Loss: 0.059762575609143824\n",
      "Evaluation phase...\n",
      "Finished evaluation for Epoch 3, Val Loss: 0.1787830765016377\n",
      "Epoch 3, Train Loss: 0.059762575609143824, Val Loss: 0.1787830765016377\n"
     ]
    }
   ],
   "source": [
    "# Optimizers\n",
    "bert_optimizer = AdamW(bert_model.parameters(), lr=2e-5)\n",
    "static_optimizer = optim.Adam(static_model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "for epoch in range(3):\n",
    "    print(f\"Starting Epoch {epoch + 1}\")\n",
    "    \n",
    "    print(\"Training phase...\")\n",
    "    train_loss = train(bert_model, train_loader, bert_optimizer, device)\n",
    "    print(f\"Finished training for Epoch {epoch + 1}, Train Loss: {train_loss}\")\n",
    "    \n",
    "    print(\"Evaluation phase...\")\n",
    "    val_loss = evaluate(bert_model, val_loader, device)\n",
    "    print(f\"Finished evaluation for Epoch {epoch + 1}, Val Loss: {val_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2b560c",
   "metadata": {},
   "source": [
    "### 7. Evaluation\n",
    "Assess the performance of the BERT model using accuracy and other metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf996c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\serem\\AppData\\Local\\Temp\\ipykernel_15260\\450977610.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.942\n",
      "Precision: 0.9301562801466332\n",
      "Recall: 0.9567374479063306\n",
      "F1 Score: 0.9432596360790452\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def get_predictions(model, data_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    real_values = []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            inputs = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(inputs, attention_mask=attention_mask)\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            predictions.extend(preds.cpu())  # Move to CPU here\n",
    "            real_values.extend(labels.cpu())  # Move to CPU here\n",
    "    return predictions, real_values\n",
    "\n",
    "# Get predictions\n",
    "predictions, real_values = get_predictions(bert_model, val_loader, device)\n",
    "\n",
    "# Convert list of tensors to single tensor\n",
    "predictions = torch.stack(predictions)\n",
    "real_values = torch.stack(real_values)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(real_values.numpy(), predictions.numpy())\n",
    "precision = precision_score(real_values.numpy(), predictions.numpy(), average='binary')\n",
    "recall = recall_score(real_values.numpy(), predictions.numpy(), average='binary')\n",
    "f1 = f1_score(real_values.numpy(), predictions.numpy(), average='binary')\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1b9955",
   "metadata": {},
   "source": [
    "### 8. Visualization\n",
    "Visualize the performance metrics using histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d93c3681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(predictions, bins=2, alpha=0.7, color='blue', label='Predictions')\n",
    "plt.hist(real_values, bins=2, alpha=0.7, color='red', label='Actual')\n",
    "plt.xlabel('Sentiments')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039a79ad",
   "metadata": {},
   "source": [
    "### 9. Conclusions and Future Work\n",
    "#### Findings\n",
    "The BERT model demonstrates strong performance in sentiment classification, with high accuracy, precision, recall, and F1 scores.\n",
    "The performance of the BERT model suggests that contextual embeddings capture the nuances of sentiment better than static embeddings.\n",
    "#### Limitations\n",
    "The training time and computational resources required for BERT are significantly higher than those for Word2Vec-based models.\n",
    "The current implementation does not include hyperparameter tuning, which could potentially improve model performance.\n",
    "#### Future Directions\n",
    "Implement and evaluate the Word2Vec-based model for a direct comparison with BERT.\n",
    "Experiment with other contextual models like RoBERTa or GPT to see if they offer performance improvements.\n",
    "Explore different preprocessing techniques and their impact on model performance.\n",
    "Conduct hyperparameter tuning to optimize model performance further."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
