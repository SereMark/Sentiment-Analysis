{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e8547d1",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of IMDb Reviews\n",
    "\n",
    "\n",
    "This notebook implements a sentiment analysis task using IMDb movie reviews. We compare the effectiveness of static embeddings (Word2Vec) and contextual embeddings (BERT) in classifying sentiments expressed in movie reviews.\n",
    "\n",
    "### 1. Setup and Installation\n",
    "\n",
    "First, we import necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e7f889",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy pandas torch transformers scikit-learn matplotlib gensim\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import re\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b50a845",
   "metadata": {},
   "source": [
    "### 2. Problem Definition and Hypothesis\n",
    "#### Problem Definition\n",
    "The objective of this project is to classify sentiments of IMDb movie reviews using vector-based text representations. Specifically, we aim to determine the efficacy of different embeddings and model architectures (static vs. contextual) in sentiment classification.\n",
    "\n",
    "#### Hypothesis\n",
    "We hypothesize that contextual embeddings (like BERT) will perform significantly better than static embeddings (like Word2Vec) for sentiment classification in movie reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8ffad1",
   "metadata": {},
   "source": [
    "### 3. Data Acquisition and Preprocessing\n",
    "We load the IMDb dataset and preprocess the text by removing HTML tags, converting to lowercase, and removing punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e49595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the IMDb movie reviews dataset\n",
    "df = pd.read_csv('C:\\Users\\serem\\Documents\\Workspaces\\Sentiment Analysis\\Data\\IMDB Dataset.csv')\n",
    "df['review'] = df['review'].apply(lambda x: re.sub('<[^>]*>', '', x).lower().replace('[^\\w\\s]', ' ').strip())\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(df['review'], df['sentiment'].map({'positive': 1, 'negative': 0}), test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenization for BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize_for_bert(texts):\n",
    "    return tokenizer(texts, padding='max_length', truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "train_encodings = tokenize_for_bert(train_texts.tolist())\n",
    "val_encodings = tokenize_for_bert(val_texts.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7889f9fa",
   "metadata": {},
   "source": [
    "### 4. Model Configuration\n",
    "#### Static Embeddings with Word2Vec\n",
    "We utilize pre-trained Word2Vec embeddings to transform text data into vectors and feed these into a simple neural network for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b01719c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained Word2Vec\n",
    "word_vectors = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "# Model for Word2Vec embeddings\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(300, 50)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(50, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "static_model = SentimentClassifier().to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca0ad9b",
   "metadata": {},
   "source": [
    "#### Contextual Embeddings with BERT\n",
    "We employ a pre-trained BERT model from the Hugging Face library, fine-tuning it on the sentiment classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8622244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT model\n",
    "bert_model = BertForSequenceClassification.from_pretrained('bert-base-uncased').to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb0d588",
   "metadata": {},
   "source": [
    "### 5. Experimentation\n",
    "We perform training and validation for both models.\n",
    "\n",
    "#### Define a Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50405a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dataloader\n",
    "class IMDbDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "train_dataset = IMDbDataset(train_encodings, train_labels.tolist())\n",
    "val_dataset = IMDbDataset(val_encodings, val_labels.tolist())\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929df21f",
   "metadata": {},
   "source": [
    "#### Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32099d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train(model, data_loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02aeebd4",
   "metadata": {},
   "source": [
    "#### 6. Model Training\n",
    "Train the BERT model and validate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7184b264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "bert_optimizer = AdamW(bert_model.parameters(), lr=2e-5)\n",
    "static_optimizer = optim.Adam(static_model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for epoch in range(3):\n",
    "    train_loss = train(bert_model, train_loader, bert_optimizer, device)\n",
    "    val_loss = evaluate(bert_model, val_loader, device)\n",
    "    print(f\"Epoch {epoch + 1}, Train Loss: {train_loss}, Val Loss: {val_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2b560c",
   "metadata": {},
   "source": [
    "### 7. Evaluation\n",
    "Assess the performance of the BERT model using accuracy and other metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf996c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, data_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    real_values = []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            inputs = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(inputs, attention_mask=attention_mask)\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            predictions.extend(preds)\n",
    "            real_values.extend(labels)\n",
    "    return predictions, real_values\n",
    "\n",
    "# Get predictions\n",
    "predictions, real_values = get_predictions(bert_model, val_loader, device)\n",
    "accuracy = accuracy_score(real_values, predictions)\n",
    "precision = precision_score(real_values, predictions, average='binary')\n",
    "recall = recall_score(real_values, predictions, average='binary')\n",
    "f1 = f1_score(real_values, predictions, average='binary')\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1b9955",
   "metadata": {},
   "source": [
    "### 8. Visualization\n",
    "Visualize the performance metrics using histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93c3681",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(predictions, bins=2, alpha=0.7, color='blue', label='Predictions')\n",
    "plt.hist(real_values, bins=2, alpha=0.7, color='red', label='Actual')\n",
    "plt.xlabel('Sentiments')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039a79ad",
   "metadata": {},
   "source": [
    "### 9. Conclusions and Future Work\n",
    "#### Findings\n",
    "The BERT model demonstrates strong performance in sentiment classification, with high accuracy, precision, recall, and F1 scores.\n",
    "The performance of the BERT model suggests that contextual embeddings capture the nuances of sentiment better than static embeddings.\n",
    "#### Limitations\n",
    "The training time and computational resources required for BERT are significantly higher than those for Word2Vec-based models.\n",
    "The current implementation does not include hyperparameter tuning, which could potentially improve model performance.\n",
    "#### Future Directions\n",
    "Implement and evaluate the Word2Vec-based model for a direct comparison with BERT.\n",
    "Experiment with other contextual models like RoBERTa or GPT to see if they offer performance improvements.\n",
    "Explore different preprocessing techniques and their impact on model performance.\n",
    "Conduct hyperparameter tuning to optimize model performance further."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
