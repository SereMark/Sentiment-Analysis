{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e8547d1",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of IMDb Reviews\n",
    "\n",
    "This notebook implements a sentiment analysis task using IMDb movie reviews. We compare the effectiveness of static embeddings (Word2Vec) and contextual embeddings (BERT) in classifying sentiments expressed in movie reviews.\n",
    "\n",
    "### 1. Setup and Installation\n",
    "\n",
    "First, we import necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e7f889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation of necessary libraries with user flags\n",
    "%pip install scipy pandas numpy transformers scikit-learn matplotlib seaborn threadpoolctl joblib gensim ipywidgets tqdm\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc, matthews_corrcoef, precision_recall_curve, average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from gensim import downloader as api\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "import itertools\n",
    "\n",
    "# Check for CUDA availability for GPU acceleration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set up random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b50a845",
   "metadata": {},
   "source": [
    "### 2. Problem Definition and Hypothesis\n",
    "#### Problem Definition\n",
    "The objective of this project is to classify sentiments of IMDb movie reviews using vector-based text representations. Specifically, we aim to determine the efficacy of different embeddings and model architectures (static vs. contextual) in sentiment classification.\n",
    "\n",
    "#### Hypothesis\n",
    "We hypothesize that contextual embeddings (like BERT) will perform significantly better than static embeddings (like Word2Vec) for sentiment classification in movie reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8ffad1",
   "metadata": {},
   "source": [
    "### 3. Data Acquisition and Preprocessing\n",
    "We load the IMDb dataset and preprocess the text by removing HTML tags, converting to lowercase, and removing punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e49595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the IMDb movie reviews dataset\n",
    "df = pd.read_csv('C:/Users/serem/Documents/Workspaces/Sentiment Analysis/IMDB Dataset.csv')\n",
    "df['review'] = df['review'].apply(lambda x: re.sub(r'<[^>]*>', '', x).lower().replace(r'[^\\w\\s]', ' ').strip())\n",
    "\n",
    "# Split data into training and testing sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(df['review'], df['sentiment'].map({'positive': 1, 'negative': 0}), test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenization for BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize_for_bert(texts):\n",
    "    return tokenizer(texts, padding='max_length', truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "train_encodings = tokenize_for_bert(train_texts.tolist())\n",
    "val_encodings = tokenize_for_bert(val_texts.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7889f9fa",
   "metadata": {},
   "source": [
    "### 4. Model Configuration\n",
    "#### Static Embeddings with Word2Vec\n",
    "We utilize pre-trained Word2Vec embeddings to transform text data into vectors and feed these into a simple neural network for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b01719c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained Word2Vec\n",
    "word_vectors = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "# Model for Word2Vec embeddings\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(300, 50)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(50, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return self.sigmoid(x)\n",
    "\n",
    "static_model = SentimentClassifier().to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca0ad9b",
   "metadata": {},
   "source": [
    "#### Contextual Embeddings with BERT\n",
    "We employ a pre-trained BERT model from the Hugging Face library, fine-tuning it on the sentiment classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8622244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT model\n",
    "bert_model = BertForSequenceClassification.from_pretrained('bert-base-uncased').to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb0d588",
   "metadata": {},
   "source": [
    "### 5. Experimentation\n",
    "We perform training and validation for both models.\n",
    "\n",
    "#### Define a Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50405a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dataloader\n",
    "class IMDbDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "train_dataset = IMDbDataset(train_encodings, train_labels.tolist())\n",
    "val_dataset = IMDbDataset(val_encodings, val_labels.tolist())\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929df21f",
   "metadata": {},
   "source": [
    "#### Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32099d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train(model, data_loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(data_loader, desc=\"Training\", leave=False)\n",
    "    for batch in progress_bar:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(data_loader, desc=\"Evaluating\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "    return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02aeebd4",
   "metadata": {},
   "source": [
    "#### 6. Model Training\n",
    "Train the BERT model and validate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7184b264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress specific UserWarning from transformers or PyTorch\n",
    "warnings.filterwarnings(\"ignore\", message=\"Torch was not compiled with flash attention.\")\n",
    "\n",
    "# Optimizers\n",
    "bert_optimizer = AdamW(bert_model.parameters(), lr=2e-5)\n",
    "static_optimizer = optim.Adam(static_model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "for epoch in range(3):\n",
    "    print(f\"Starting Epoch {epoch + 1}\")\n",
    "    \n",
    "    print(\"Training phase...\")\n",
    "    train_loss = train(bert_model, train_loader, bert_optimizer, device)\n",
    "    print(f\"Finished training for Epoch {epoch + 1}, Train Loss: {train_loss:.4f}\")\n",
    "    \n",
    "    print(\"Evaluation phase...\")\n",
    "    val_loss = evaluate(bert_model, val_loader, device)\n",
    "    print(f\"Finished evaluation for Epoch {epoch + 1}, Val Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2b560c",
   "metadata": {},
   "source": [
    "### 7. Evaluation\n",
    "Assess the performance of the BERT model using accuracy and other metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf996c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get model predictions\n",
    "def get_predictions(model, data_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    real_values = []\n",
    "    prediction_scores = []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            inputs = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(inputs, attention_mask=attention_mask)\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            real_values.extend(labels.cpu().numpy())\n",
    "            prediction_scores.extend(outputs.logits[:,1].cpu().numpy())\n",
    "    return predictions, real_values, prediction_scores\n",
    "\n",
    "# Evaluate model\n",
    "predictions, real_values, prediction_scores = get_predictions(bert_model, val_loader, device)\n",
    "\n",
    "# Metrics\n",
    "cm = confusion_matrix(real_values, predictions)\n",
    "accuracy = accuracy_score(real_values, predictions)\n",
    "precision = precision_score(real_values, predictions, average='binary')\n",
    "recall = recall_score(real_values, predictions, average='binary')\n",
    "f1 = f1_score(real_values, predictions, average='binary')\n",
    "mcc = matthews_corrcoef(real_values, predictions)\n",
    "\n",
    "# Output metrics\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "print(f\"Matthews Correlation Coefficient: {mcc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1b9955",
   "metadata": {},
   "source": [
    "### 8. Visualization\n",
    "This section leverages multiple visual tools to comprehensively display the model's performance metrics, helping to illustrate its strengths and pinpoint areas that may require improvement. By examining these visualizations, we can gain deeper insights into the model's behavior in various scenarios.\n",
    "\n",
    "#### Confusion Matrix\n",
    "The Confusion Matrix is an essential tool for understanding the performance of classification models at a granular level. It distinguishes between the counts of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN). In this normalized version, each value in the matrix represents the proportion of predictions for actual class labels, providing clarity on the model's precision in classifying each category. This helps identify if the model is biased or particularly weak in recognizing one class over another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93c3681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized Confusion Matrix visualization\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm_normalized, annot=True, fmt=\".2f\", cmap='Blues', xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'])\n",
    "plt.title('Normalized Confusion Matrix')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f537184",
   "metadata": {},
   "source": [
    "#### ROC Curve\n",
    "The Receiver Operating Characteristic (ROC) Curve is a powerful diagnostic tool for binary classifiers. It plots the True Positive Rate (TPR) against the False Positive Rate (FPR) at various threshold levels, providing a visual representation of the trade-off between sensitivity and specificity. The Area Under the Curve (AUC) metric summarizes the overall ability of the model to discriminate between the classes at all thresholds. Points along the curve represent different thresholds; a higher curve indicates better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f401c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve with threshold annotations\n",
    "fpr, tpr, thresholds = roc_curve(real_values, prediction_scores)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "threshold_indices = np.searchsorted(thresholds, np.linspace(0.1, 0.9, 9))\n",
    "threshold_indices = np.clip(threshold_indices, 0, len(thresholds) - 1)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "for idx in np.unique(threshold_indices):  # Ensure unique indices to avoid duplicate labels\n",
    "    plt.text(fpr[idx], tpr[idx], f'Thresh={thresholds[idx]:.2f}', fontsize=9)\n",
    "plt.fill_between(fpr, tpr, alpha=0.2, color='orange')\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14629977",
   "metadata": {},
   "source": [
    "#### Precision-Recall Curve\n",
    "The Precision-Recall Curve is another crucial measure for models, especially in scenarios with imbalanced classes. It focuses on the relationship between precision (accuracy of positive predictions) and recall (ability to find all positive instances), which is crucial for models where the cost of false negatives is high. The area under the curve (AP) is a single value metric that quantifies the performance depicted by the curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba5a235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall Curve visualization\n",
    "precision, recall, _ = precision_recall_curve(real_values, prediction_scores)\n",
    "ap_score = average_precision_score(real_values, prediction_scores)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, color='green', lw=2, label=f'Precision-Recall curve (AP = {ap_score:.2f})')\n",
    "plt.fill_between(recall, precision, alpha=0.2, color='green')\n",
    "plt.xlim([0.0, 1.05])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682e0c1c",
   "metadata": {},
   "source": [
    "#### Histograms of Predictions vs. Actual Values\n",
    "Histograms are straightforward yet effective for comparing the distribution of predicted versus actual values. This visualization helps assess how well the predictions align with the true labels and can indicate if the model tends to over-predict one class or is generally balanced. It is particularly useful for identifying biases in predictive behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901ad987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram for Predictions vs Actual Values\n",
    "plt.figure(figsize=(10, 6))\n",
    "bins = np.array([0, 1]) - 0.5  # Center bins on integer values\n",
    "plt.hist(predictions, bins=bins, alpha=0.5, label='Predictions', color='blue', rwidth=0.8)\n",
    "plt.hist(real_values, bins=bins, alpha=0.5, label='Actual Values', color='red', rwidth=0.8)\n",
    "plt.xticks([0, 1], ['Negative', 'Positive'])\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.title('Comparison of Predictions and Actual Values')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039a79ad",
   "metadata": {},
   "source": [
    "### 9. Conclusions and Future Work\n",
    "#### Findings\n",
    "The BERT model demonstrates strong performance in sentiment classification, with high accuracy, precision, recall, and F1 scores.\n",
    "The performance of the BERT model suggests that contextual embeddings capture the nuances of sentiment better than static embeddings.\n",
    "#### Limitations\n",
    "The training time and computational resources required for BERT are significantly higher than those for Word2Vec-based models.\n",
    "The current implementation does not include hyperparameter tuning, which could potentially improve model performance.\n",
    "#### Future Directions\n",
    "Implement and evaluate the Word2Vec-based model for a direct comparison with BERT.\n",
    "Experiment with other contextual models like RoBERTa or GPT to see if they offer performance improvements.\n",
    "Explore different preprocessing techniques and their impact on model performance.\n",
    "Conduct hyperparameter tuning to optimize model performance further."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
